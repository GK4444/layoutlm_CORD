{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"layoutlm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4SiUcS6zENfudTokwxdvp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"24opowDSuiP0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606484414108,"user_tz":-60,"elapsed":11638,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"d684a0ee-7551-4937-d0ee-56371d2ca77d"},"source":["!pip install transformers\n","!pip install seqeval"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 10.4MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 43.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 51.4MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=d6d0eb2793e39c0d2cc7a90964c76f439399b417b001027673104f07a9498406\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n","Collecting seqeval\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=3833544a2170473658b9851a53ce44f57261c1ba4b5ff672ba28194cfbd81749\n","  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNFjqj90Hp7N","executionInfo":{"status":"ok","timestamp":1606484445007,"user_tz":-60,"elapsed":23676,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"b8598e54-e5de-485f-ced6-88a9b95af543"},"source":["import numpy as np\n","import pickle\n","import random\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","#### METRICS ####\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score)\n","\n","\n","##### UTILS ######\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertConfig, BertModel, BertPreTrainedModel, get_linear_schedule_with_warmup, AdamW, BertTokenizerFast\n","from torch.nn import LayerNorm as BertLayerNorm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9vtQBkYPwe5B"},"source":["Load the data"]},{"cell_type":"code","metadata":{"id":"8C1po-tlH75q","executionInfo":{"status":"ok","timestamp":1606484446691,"user_tz":-60,"elapsed":1677,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["## Import Data\n","path = '/content/drive/My Drive/layoutlm/'\n","train = pickle.load(open(path + 'train.pkl', 'rb'))\n","val = pickle.load(open(path + 'val.pkl', 'rb'))\n","test = pickle.load(open(path + 'test.pkl', 'rb'))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEo2CZ3mwS84"},"source":["This is simply a code to see if there are some negative bounding boxes (I find some, they were close to zero, then I set them to zero)"]},{"cell_type":"code","metadata":{"id":"eKM5ubNfhUEs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606484446692,"user_tz":-60,"elapsed":1668,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"0059ae52-38ea-45b2-90ce-fb468952aa01"},"source":["for index, elem in enumerate(train[2]):\n","  for i,e in enumerate(elem):\n","    if not all(np.array(e) >= 0):\n","      print(e)\n","      print(index, i)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[-4, 426, 213, 396]\n","669 0\n","[-3, 466, 112, 435]\n","669 3\n","[-1, 529, 223, 508]\n","669 6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zf7Tl8-ZjDIr","executionInfo":{"status":"ok","timestamp":1606484446693,"user_tz":-60,"elapsed":1661,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["train[2][669][0] = [0, 426, 213, 396]\n","train[2][669][3] = [0, 466, 112, 435]\n","train[2][669][6] = [0, 529, 223, 508]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"VThQLP2Tiu_-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606484446693,"user_tz":-60,"elapsed":1653,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"f878f581-44ca-4341-9055-0cc482ceb150"},"source":["for index, elem in enumerate(test[2]):\n","  for i,e in enumerate(elem):\n","    if not all(np.array(e) >= 0):\n","      print(e)\n","      print(index, i)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[-3, 891, 106, 862]\n","88 17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RWTTF_GajZz2","executionInfo":{"status":"ok","timestamp":1606484446694,"user_tz":-60,"elapsed":1240,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["test[2][88][17] = [0, 891, 106, 862]"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnAtbMfw8oaW"},"source":["# Labels Analysis"]},{"cell_type":"code","metadata":{"id":"eMNyf22lvUqR","executionInfo":{"status":"ok","timestamp":1606484448118,"user_tz":-60,"elapsed":776,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["all_labels = [item for sublist in train[1] for item in sublist] + [item for sublist in val[1] for item in sublist] + [item for sublist in test[1] for item in sublist]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fg8kIOrFv0F3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606484448463,"user_tz":-60,"elapsed":369,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"df92c214-5d62-42cb-e302-53b4fa9ba157"},"source":["from collections import Counter\n","Counter(all_labels)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'menu.cnt': 2429,\n","         'menu.discountprice': 403,\n","         'menu.etc': 19,\n","         'menu.itemsubtotal': 7,\n","         'menu.nm': 6599,\n","         'menu.num': 109,\n","         'menu.price': 2585,\n","         'menu.sub_cnt': 189,\n","         'menu.sub_etc': 10,\n","         'menu.sub_nm': 822,\n","         'menu.sub_price': 160,\n","         'menu.sub_unitprice': 14,\n","         'menu.unitprice': 750,\n","         'menu.vatyn': 9,\n","         'sub_total.discount_price': 191,\n","         'sub_total.etc': 283,\n","         'sub_total.othersvc_price': 6,\n","         'sub_total.service_price': 353,\n","         'sub_total.subtotal_price': 1483,\n","         'sub_total.tax_price': 1283,\n","         'total.cashprice': 1397,\n","         'total.changeprice': 1299,\n","         'total.creditcardprice': 411,\n","         'total.emoneyprice': 129,\n","         'total.menuqty_cnt': 630,\n","         'total.menutype_cnt': 130,\n","         'total.total_etc': 89,\n","         'total.total_price': 2120,\n","         'void_menu.nm': 3,\n","         'void_menu.price': 1})"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"ZlGPB3IPwjB_"},"source":["As we can see there are some labels that contains few examples, I decided to replace them by the \"neutral\" label \"O\""]},{"cell_type":"code","metadata":{"id":"_ic2qLS88x8P","executionInfo":{"status":"ok","timestamp":1606484450776,"user_tz":-60,"elapsed":830,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["replacing_labels = {'menu.etc': 'O', 'mneu.itemsubtotal': 'O', 'menu.sub_etc': 'O', 'menu.sub_unitprice': 'O', 'menu.vatyn': 'O',\n","                  'void_menu.nm': 'O', 'void_menu.price': 'O', 'sub_total.othersvc_price': 'O'}"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"3c3XitV9AN8W","executionInfo":{"status":"ok","timestamp":1606484451295,"user_tz":-60,"elapsed":540,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["def replace_elem(elem):\n","  try:\n","    return replacing_labels[elem]\n","  except KeyError:\n","    return elem\n","def replace_list(ls):\n","  return [replace_elem(elem) for elem in ls]\n","train[1] = [replace_list(ls) for ls in train[1]]\n","val[1] = [replace_list(ls) for ls in val[1]]\n","test[1] = [replace_list(ls) for ls in test[1]]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCjWSztSBG7-","executionInfo":{"status":"ok","timestamp":1606484453474,"user_tz":-60,"elapsed":541,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"6911af50-52f5-40a9-ca94-44fcd2d500bc"},"source":["all_labels = [item for sublist in train[1] for item in sublist] + [item for sublist in val[1] for item in sublist] + [item for sublist in test[1] for item in sublist]\n","Counter(all_labels)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'O': 62,\n","         'menu.cnt': 2429,\n","         'menu.discountprice': 403,\n","         'menu.itemsubtotal': 7,\n","         'menu.nm': 6599,\n","         'menu.num': 109,\n","         'menu.price': 2585,\n","         'menu.sub_cnt': 189,\n","         'menu.sub_nm': 822,\n","         'menu.sub_price': 160,\n","         'menu.unitprice': 750,\n","         'sub_total.discount_price': 191,\n","         'sub_total.etc': 283,\n","         'sub_total.service_price': 353,\n","         'sub_total.subtotal_price': 1483,\n","         'sub_total.tax_price': 1283,\n","         'total.cashprice': 1397,\n","         'total.changeprice': 1299,\n","         'total.creditcardprice': 411,\n","         'total.emoneyprice': 129,\n","         'total.menuqty_cnt': 630,\n","         'total.menutype_cnt': 130,\n","         'total.total_etc': 89,\n","         'total.total_price': 2120})"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"gJ_8wwhWwvUY"},"source":["Now we have to save all the unique labels in a list. (mandatory for the tokenclassification)"]},{"cell_type":"code","metadata":{"id":"sU67bqUsBM65","executionInfo":{"status":"ok","timestamp":1606484455743,"user_tz":-60,"elapsed":508,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["labels = list(set(all_labels))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_A9s4x7RLdXt"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"E8lMOtu7xFle"},"source":["This is only the class config (same as in bert (BertConfig))"]},{"cell_type":"code","metadata":{"id":"bckR_O9nIxEp","executionInfo":{"status":"ok","timestamp":1606484456997,"user_tz":-60,"elapsed":529,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n","\n","LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n","\n","\n","class LayoutlmConfig(BertConfig):\n","    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n","    model_type = \"bert\"\n","\n","    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_2d_position_embeddings = max_2d_position_embeddings"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5S5qq0mxOXF"},"source":["This is the NEW embedding that takes into account the position embedding, the rest is the same as in bert, we sum the token embedding, positional embedding, all the position embedding and the token type embedding"]},{"cell_type":"code","metadata":{"id":"HNrK6wphxCGO","executionInfo":{"status":"ok","timestamp":1606484460504,"user_tz":-60,"elapsed":675,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["class LayoutlmEmbeddings(nn.Module):\n","    def __init__(self, config):\n","        super(LayoutlmEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size, config.hidden_size, padding_idx=0\n","        )\n","        self.position_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.x_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.y_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.h_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.w_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.token_type_embeddings = nn.Embedding(\n","            config.type_vocab_size, config.hidden_size\n","        )\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","    ):\n","        seq_length = input_ids.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device\n","            )\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n","        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n","        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n","        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n","        h_position_embeddings = self.h_position_embeddings(\n","            bbox[:, :, 1] - bbox[:, :, 3]\n","        )\n","        w_position_embeddings = self.w_position_embeddings(\n","            bbox[:, :, 2] - bbox[:, :, 0]\n","        )\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = (\n","            words_embeddings\n","            + position_embeddings\n","            + left_position_embeddings\n","            + upper_position_embeddings\n","            + right_position_embeddings\n","            + lower_position_embeddings\n","            + h_position_embeddings\n","            + w_position_embeddings\n","            + token_type_embeddings\n","        )\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1M6Y4A23xpgi"},"source":["This is the LayoutlmModel (equivalent to BertModel except that this one has an extra input called bbox."]},{"cell_type":"code","metadata":{"id":"kVMlm0gPxCQ4","executionInfo":{"status":"ok","timestamp":1606484462129,"user_tz":-60,"elapsed":517,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["class LayoutlmModel(BertModel):\n","\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmModel, self).__init__(config)\n","        self.embeddings = LayoutlmEmbeddings(config)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = (\n","                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                )\n","                head_mask = head_mask.expand(\n","                    self.config.num_hidden_layers, -1, -1, -1, -1\n","                )\n","            elif head_mask.dim() == 2:\n","                head_mask = (\n","                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n","                )  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(\n","                dtype=next(self.parameters()).dtype\n","            )  # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        embedding_output = self.embeddings(\n","            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output, extended_attention_mask, head_mask=head_mask\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HfJwXJMyK4-"},"source":["This is the equivalent of BertForTokenClassification except that the base model is the LayoutlmModel."]},{"cell_type":"code","metadata":{"id":"5oGyFHOvx7S1","executionInfo":{"status":"ok","timestamp":1606484464881,"user_tz":-60,"elapsed":533,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["class LayoutlmForTokenClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxlPHOdsyar7"},"source":["# Some aux functions"]},{"cell_type":"code","metadata":{"id":"r5uOfQyIXTOA","executionInfo":{"status":"ok","timestamp":1606484501373,"user_tz":-60,"elapsed":530,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["def set_seed(seed): ## for reproductibility\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9msa1C8RytLs"},"source":["This is for making the structure of our data match the one needed by the model (mandatory)"]},{"cell_type":"code","metadata":{"id":"ipRLhsY1yo-I","executionInfo":{"status":"ok","timestamp":1606484509727,"user_tz":-60,"elapsed":709,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["class CordDataset(Dataset):\n","    def __init__(self, examples, tokenizer, labels, pad_token_label_id):\n","        features = convert_examples_to_features(\n","            examples,\n","            labels,\n","            max_seq_length,\n","            tokenizer,\n","            cls_token_at_end=False,\n","            # xlnet has a cls token at the end\n","            cls_token=tokenizer.cls_token,\n","            cls_token_segment_id=0,\n","            sep_token=tokenizer.sep_token,\n","            sep_token_extra=False,\n","            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","            pad_on_left=False,\n","            # pad on the left for xlnet\n","            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","            pad_token_segment_id=0,\n","            pad_token_label_id=pad_token_label_id,\n","        )\n","\n","        self.features = features\n","        # Convert to Tensors and build dataset\n","        self.all_input_ids = torch.tensor(\n","            [f.input_ids for f in features], dtype=torch.long\n","        )\n","        self.all_input_mask = torch.tensor(\n","            [f.input_mask for f in features], dtype=torch.long\n","        )\n","        self.all_segment_ids = torch.tensor(\n","            [f.segment_ids for f in features], dtype=torch.long\n","        )\n","        self.all_label_ids = torch.tensor(\n","            [f.label_ids for f in features], dtype=torch.long\n","        )\n","        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        return (\n","            self.all_input_ids[index],\n","            self.all_input_mask[index],\n","            self.all_segment_ids[index],\n","            self.all_label_ids[index],\n","            self.all_bboxes[index],\n","        )\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_ids,\n","        input_mask,\n","        segment_ids,\n","        label_ids,\n","        boxes\n","    ):\n","        assert (\n","            0 <= all(boxes) <= 1000\n","        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n","            boxes\n","        )\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.boxes = boxes\n","\n","def convert_examples_to_features(\n","    examples,\n","    label_list,\n","    max_seq_length,\n","    tokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    cls_token_box=[0, 0, 0, 0],\n","    sep_token_box=[1000, 1000, 1000, 1000],\n","    pad_token_box=[0, 0, 0, 0],\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-1,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for i in range(len(examples[0])):\n","        width, height = 1000, 1000\n","        words = examples[0]\n","        labels = examples[1]\n","        boxes = examples[2]\n","\n","        tokens = []\n","        token_boxes = []\n","        label_ids = []\n","        for word, label, box in zip(\n","            words[i], labels[i], boxes[i]\n","        ):\n","            if len(word) < 1: # SKIP EMPTY WORD\n","              continue\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            token_boxes.extend([box] * len(word_tokens))\n","            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","            label_ids.extend(\n","                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 3 if sep_token_extra else 2\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        token_boxes += [sep_token_box]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            token_boxes += [sep_token_box]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            token_boxes += [cls_token_box]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            token_boxes = [cls_token_box] + token_boxes\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = (\n","                [0 if mask_padding_with_zero else 1] * padding_length\n","            ) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","            token_boxes += [pad_token_box] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","        assert len(token_boxes) == max_seq_length\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                label_ids=label_ids,\n","                boxes=token_boxes,\n","            )\n","        )\n","    return features\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6shAbUJzWeV"},"source":["This function is for the evaluation (loss, f1, precision , recall)"]},{"cell_type":"code","metadata":{"id":"lK-jb-vXzahD","executionInfo":{"status":"ok","timestamp":1606484516972,"user_tz":-60,"elapsed":733,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["def results(preds, out_label_ids, labels, loss_):\n","  preds = np.argmax(preds, axis=2)\n","\n","  label_map = {i: label for i, label in enumerate(labels)}\n","\n","  out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","  preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","  for i in range(out_label_ids.shape[0]):\n","      for j in range(out_label_ids.shape[1]):\n","          if out_label_ids[i, j] != pad_token_label_id:\n","              out_label_list[i].append(label_map[out_label_ids[i][j]])\n","              preds_list[i].append(label_map[preds[i][j]])\n","\n","  results = {\n","      \"loss\": loss_,\n","      \"precision\": precision_score(out_label_list, preds_list),\n","      \"recall\": recall_score(out_label_list, preds_list),\n","      \"f1\": f1_score(out_label_list, preds_list),\n","  }\n","  return results"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bahaNkwL0Tor"},"source":["# Load models"]},{"cell_type":"code","metadata":{"id":"Po75bopj08eN","executionInfo":{"status":"ok","timestamp":1606484518550,"user_tz":-60,"elapsed":545,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1UBWYX1XyWo"},"source":["model_path = '/content/drive/My Drive/layoutlm/'\n","num_labels = len(labels)\n","config_class, model_class, tokenizer_class = LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizerFast\n","config = config_class.from_pretrained(model_path, num_labels=num_labels)\n","tokenizer = tokenizer_class.from_pretrained(model_path, do_lower_case=True)\n","model = model_class.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path), config=config)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sf3XgoQS0YXY"},"source":["# Generate our training / validation set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwSEe05VCGYb","executionInfo":{"status":"ok","timestamp":1606483997510,"user_tz":-60,"elapsed":616,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"812f7498-237b-45f8-cb44-7b9ed5d96a68"},"source":["sum_ = []\n","for x in train[0]:\n","  sum_.append(len(x))\n","print(max(sum_))\n","print(min(sum_))\n","print(sum(sum_)/len(sum_))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["135\n","5\n","24.21375\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oO_wnwuiaGYp","executionInfo":{"status":"ok","timestamp":1606484571047,"user_tz":-60,"elapsed":1606,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["max_seq_length = 150\n","pad_token_label_id = CrossEntropyLoss().ignore_index\n","train_dataset = CordDataset(train, tokenizer, labels, pad_token_label_id)\n","validation_dataset = CordDataset(val, tokenizer, labels, pad_token_label_id)\n","model_type = 'layoutlm'"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnsAw1XcOVi1","executionInfo":{"status":"ok","timestamp":1606484587328,"user_tz":-60,"elapsed":560,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["train_batch_size = 8\n","learning_rate = 1e-4\n","adam_epsilon = 1e-8\n","weight_decay = 0.0\n","num_train_epochs = 4 ## To fine-tune (adding drop out so that It can lead to overfit less)\n","max_steps = 0\n","gradient_accumulation_steps = 1\n","max_grad_norm = 1.0\n","warmup_steps = 0\n","seed = 42\n","\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=train_sampler,\n","        batch_size=train_batch_size,\n","        collate_fn=None,\n","    )\n","valid_sampler = RandomSampler(validation_dataset)\n","valid_dataloader = DataLoader(\n","        validation_dataset,\n","        sampler=valid_sampler,\n","        batch_size=train_batch_size,\n","        collate_fn=None,\n","    )\n","if max_steps > 0:\n","    t_total = max_steps\n","    num_train_epochs = (\n","        args.max_steps\n","        // (len(train_dataloader) // gradient_accumulation_steps)\n","        + 1\n","    )\n","else:\n","    t_total = (\n","        len(train_dataloader)\n","        // gradient_accumulation_steps\n","        * num_train_epochs\n","    )\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","optimizer = AdamW(\n","        optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n","    )\n","scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n","    )"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuXqu7AJ0qhC"},"source":["# TRAIN"]},{"cell_type":"code","metadata":{"id":"5BfPvHDGPErD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606484920659,"user_tz":-60,"elapsed":328856,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"2c3c507c-e936-45de-d820-164d1bac1909"},"source":["global_step = 0\n","model.zero_grad()\n","train_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\n","set_seed(seed)\n","for _ in train_iterator:\n","  #epoch_iterator = tqdm(\n","      #train_dataloader, desc=\"Iteration\")\n","  tr_loss = 0.0\n","  nb_train_steps = 0\n","  preds_train = None\n","  out_label_ids = None\n","  for step, batch in enumerate(train_dataloader):\n","      model.train()\n","      inputs = {\n","          \"input_ids\": batch[0].to(device),\n","          \"attention_mask\": batch[1].to(device),\n","          \"labels\": batch[3].to(device),\n","      }\n","      if model_type in [\"layoutlm\"]:\n","          inputs[\"bbox\"] = batch[4].to(device)\n","      inputs[\"token_type_ids\"] = (\n","          batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n","      outputs = model(**inputs)\n","      # model outputs are always tuple in pytorch-transformers (see doc)\n","      loss, logits = outputs\n","      loss.backward()\n","\n","      tr_loss += loss.item()\n","      #if (step+1) % 25 == 0:\n","        #print(f\"Train Epoch : {step+1}/{len(train_dataloader)}\")\n","\n","      if (step + 1) % gradient_accumulation_steps == 0:\n","          torch.nn.utils.clip_grad_norm_(\n","                  model.parameters(), max_grad_norm\n","              )\n","          optimizer.step()\n","          scheduler.step()  # Update learning rate schedule\n","          model.zero_grad()\n","          global_step += 1\n","      nb_train_steps += 1\n","      if preds_train is None:\n","          preds_train = logits.detach().cpu().numpy()\n","          out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","      else:\n","          preds_train = np.append(preds_train, logits.detach().cpu().numpy(), axis=0)\n","          out_label_ids = np.append(\n","              out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n","  res = results(preds_train, out_label_ids, labels, tr_loss/len(train_dataloader))\n","  print('Train Results', res)\n","\n","  ###### EVALUATION #######\n","\n","  #epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\")\n","  eval_loss = 0.0\n","  nb_eval_steps = 0\n","  preds_val = None\n","  out_label_ids = None\n","  model.eval()\n","  for step, batch in enumerate(valid_dataloader):\n","    with torch.no_grad():\n","      inputs = {\n","          \"input_ids\": batch[0].to(device),\n","          \"attention_mask\": batch[1].to(device),\n","          \"labels\": batch[3].to(device),\n","      }\n","      if model_type in [\"layoutlm\"]:\n","          inputs[\"bbox\"] = batch[4].to(device)\n","      inputs[\"token_type_ids\"] = (\n","          batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n","      # model outputs are always tuple in pytorch-transformers (see doc)\n","      outputs = model(**inputs)\n","      tmp_eval_loss, logits = outputs[:2]\n","      eval_loss += tmp_eval_loss.item()\n","    nb_eval_steps += 1\n","    if preds_val is None:\n","      preds_val = logits.detach().cpu().numpy()\n","      out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","    else:\n","      preds_val = np.append(preds_val, logits.detach().cpu().numpy(), axis=0)\n","      out_label_ids = np.append(\n","          out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","      )\n","  eval_loss = eval_loss / nb_eval_steps\n","  res = results(preds_val, out_label_ids, labels, eval_loss)\n","  print('Validation results',res)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Results {'loss': 0.7849363887310028, 'precision': 0.7121579183490354, 'recall': 0.7402536840141765, 'f1': 0.7259340558832944}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  25%|██▌       | 1/4 [01:18<03:55, 78.51s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation results {'loss': 0.33114434549441707, 'precision': 0.8516179952644041, 'recall': 0.8736842105263158, 'f1': 0.8625099920063949}\n","Train Results {'loss': 0.2368683361634612, 'precision': 0.8971851581063791, 'recall': 0.9155941055773177, 'f1': 0.9062961595273266}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  50%|█████     | 2/4 [02:40<02:38, 79.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation results {'loss': 0.1977280696065953, 'precision': 0.9284552845528455, 'recall': 0.9246963562753037, 'f1': 0.9265720081135903}\n","Train Results {'loss': 0.1343210445996374, 'precision': 0.9408180223432739, 'recall': 0.9503823913448983, 'f1': 0.9455760218995035}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  75%|███████▌  | 3/4 [04:03<01:20, 80.63s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation results {'loss': 0.1667864156815295, 'precision': 0.9376012965964343, 'recall': 0.9368421052631579, 'f1': 0.9372215471850952}\n","Train Results {'loss': 0.06489855873398483, 'precision': 0.9724003345413995, 'recall': 0.9759373251259094, 'f1': 0.9741656193269097}\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 4/4 [05:27<00:00, 82.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation results {'loss': 0.17203735510030618, 'precision': 0.9577579203899269, 'recall': 0.9546558704453442, 'f1': 0.9562043795620438}\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vXrHW2awvKQt","executionInfo":{"status":"ok","timestamp":1606482523689,"user_tz":-60,"elapsed":7675,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["torch.save(model.state_dict(), '/content/drive/My Drive/layoutlm/customllm.pt')"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLdpqOqHnGxW"},"source":["# TEST"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGDPJFgP4pfX","executionInfo":{"status":"ok","timestamp":1606484927438,"user_tz":-60,"elapsed":591,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"30bccb8f-cd72-4992-bb59-3dc974cec7e1"},"source":["for index, e in enumerate(test[2]):\n","  for index_,l in enumerate(e):\n","    if l[1] < l[3]:\n","      print(index, index_)\n","      print(l)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["54 15\n","[105, 726, 498, 738]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yhBHNhyV5D40","executionInfo":{"status":"ok","timestamp":1606484928610,"user_tz":-60,"elapsed":661,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["test[2][54][15] = [105, 738, 498, 726]"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgUBgVEM4rHQ","executionInfo":{"status":"ok","timestamp":1606484929546,"user_tz":-60,"elapsed":637,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["test_dataset = CordDataset(test, tokenizer, labels, pad_token_label_id)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xrgr8-F_pu0s","executionInfo":{"status":"ok","timestamp":1606484931110,"user_tz":-60,"elapsed":618,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["def results_test(preds, out_label_ids, labels):\n","  preds = np.argmax(preds, axis=2)\n","\n","  label_map = {i: label for i, label in enumerate(labels)}\n","\n","  out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","  preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","  for i in range(out_label_ids.shape[0]):\n","      for j in range(out_label_ids.shape[1]):\n","          if out_label_ids[i, j] != pad_token_label_id:\n","              out_label_list[i].append(label_map[out_label_ids[i][j]])\n","              preds_list[i].append(label_map[preds[i][j]])\n","\n","  results = {\n","      \"precision\": precision_score(out_label_list, preds_list),\n","      \"recall\": recall_score(out_label_list, preds_list),\n","      \"f1\": f1_score(out_label_list, preds_list),\n","  }\n","  return results, classification_report(out_label_list, preds_list)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifR-pkUu8a9A","executionInfo":{"status":"ok","timestamp":1606484933653,"user_tz":-60,"elapsed":589,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["val_r, class_r = results_test(preds_val, out_label_ids, labels)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P83YLa0X9sFs","executionInfo":{"status":"ok","timestamp":1606484949325,"user_tz":-60,"elapsed":474,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"44eff4e6-6047-4319-9848-c7d613c8eddd"},"source":["print(class_r)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["                         precision    recall  f1-score   support\n","\n","                enu.cnt       0.99      0.98      0.98       213\n","      enu.discountprice       0.75      0.75      0.75         4\n","                 enu.nm       0.98      0.98      0.98       221\n","                enu.num       0.80      1.00      0.89         4\n","              enu.price       0.98      0.98      0.98       222\n","            enu.sub_cnt       0.94      0.97      0.95        32\n","             enu.sub_nm       0.96      0.96      0.96        45\n","          enu.sub_price       0.86      0.80      0.83        15\n","          enu.unitprice       0.88      0.96      0.92        52\n","         otal.cashprice       0.95      0.94      0.95        66\n","       otal.changeprice       0.97      0.95      0.96        66\n","   otal.creditcardprice       0.88      0.93      0.90        15\n","       otal.emoneyprice       0.80      1.00      0.89         4\n","       otal.menuqty_cnt       0.85      0.88      0.87        26\n","      otal.menutype_cnt       1.00      0.67      0.80         3\n","         otal.total_etc       0.00      0.00      0.00         5\n","       otal.total_price       0.97      0.96      0.97       102\n","ub_total.discount_price       0.75      1.00      0.86         6\n","           ub_total.etc       0.00      0.00      0.00         1\n"," ub_total.service_price       0.92      0.92      0.92        13\n","ub_total.subtotal_price       0.97      0.90      0.93        71\n","     ub_total.tax_price       0.90      0.96      0.93        49\n","\n","              micro avg       0.96      0.95      0.96      1235\n","              macro avg       0.82      0.84      0.83      1235\n","           weighted avg       0.96      0.95      0.95      1235\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9T_AvnJ-nIKx","executionInfo":{"status":"ok","timestamp":1606485020417,"user_tz":-60,"elapsed":24384,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}}},"source":["test_sampler = RandomSampler(test_dataset)\n","test_dataloader = DataLoader(\n","        test_dataset,\n","        sampler=test_sampler,\n","        batch_size=2,\n","        collate_fn=None,\n","    )\n","nb_eval_steps = 0\n","preds_test = None\n","out_label_ids = None\n","model.eval()\n","for step, batch in enumerate(train_dataloader):\n","  with torch.no_grad():\n","    inputs = {\n","        \"input_ids\": batch[0].to(device),\n","        \"attention_mask\": batch[1].to(device),\n","        \"labels\": batch[3].to(device),\n","    }\n","    if model_type in [\"layoutlm\"]:\n","        inputs[\"bbox\"] = batch[4].to(device)\n","    inputs[\"token_type_ids\"] = (\n","        batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n","    # model outputs are always tuple in pytorch-transformers (see doc)\n","    outputs = model(**inputs)\n","    _, logits = outputs[:2]\n","  if preds_test is None:\n","    preds_test = logits.detach().cpu().numpy()\n","    out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","  else:\n","    preds_test = np.append(preds_test, logits.detach().cpu().numpy(), axis=0)\n","    out_label_ids = np.append(\n","        out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","    )\n","\n","res, report = results_test(preds_test, out_label_ids, labels)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"audIOKGL99ir","executionInfo":{"status":"ok","timestamp":1606485025637,"user_tz":-60,"elapsed":546,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"4e8a4dd9-7d89-41bf-f48a-182c11a5eeee"},"source":["res"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'f1': 0.984280983254816,\n"," 'precision': 0.9845105906503686,\n"," 'recall': 0.9840514829322887}"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeJadGsH-CEI","executionInfo":{"status":"ok","timestamp":1606485033560,"user_tz":-60,"elapsed":559,"user":{"displayName":"Omar Souaidi","photoUrl":"","userId":"18313246010754186886"}},"outputId":"dcffaa67-7ffa-49ee-9454-4290da57a949"},"source":["print(report)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["                         precision    recall  f1-score   support\n","\n","                enu.cnt       0.99      0.99      0.99      1887\n","      enu.discountprice       0.97      0.95      0.96        82\n","       enu.itemsubtotal       0.00      0.00      0.00         1\n","                 enu.nm       0.99      1.00      0.99      2060\n","                enu.num       0.98      0.99      0.98        90\n","              enu.price       1.00      0.99      0.99      2058\n","            enu.sub_cnt       0.98      0.95      0.96       140\n","             enu.sub_nm       0.96      0.96      0.96       273\n","          enu.sub_price       0.95      0.88      0.91       121\n","          enu.unitprice       0.97      0.99      0.98       612\n","         otal.cashprice       0.99      0.99      0.99       527\n","       otal.changeprice       0.99      0.98      0.99       505\n","   otal.creditcardprice       0.98      0.98      0.98       119\n","       otal.emoneyprice       0.92      0.92      0.92        50\n","       otal.menuqty_cnt       0.95      0.99      0.97       230\n","      otal.menutype_cnt       0.94      0.79      0.86        42\n","         otal.total_etc       0.81      0.85      0.83        26\n","       otal.total_price       0.98      0.98      0.98       780\n","ub_total.discount_price       0.92      0.94      0.93        65\n","           ub_total.etc       0.94      0.89      0.91        72\n"," ub_total.service_price       1.00      0.99      0.99        91\n","ub_total.subtotal_price       0.97      0.97      0.97       536\n","     ub_total.tax_price       0.97      0.97      0.97       355\n","\n","              micro avg       0.98      0.98      0.98     10722\n","              macro avg       0.92      0.91      0.91     10722\n","           weighted avg       0.98      0.98      0.98     10722\n","\n"],"name":"stdout"}]}]}